{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Yc_h16yYEI"
      },
      "source": [
        "# Multimedia Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Rgjy3aHxCa"
      },
      "source": [
        "## Loading data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj-jcK9D81uT"
      },
      "source": [
        "For this assignment, we will use the [Paris Dataset](https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/). This dataset comprises 6412 images taken at 12 well-known landmarks in the City of Love. These images were collected by querying the name for each landmark on [Flickr](https://www.flickr.com/).\n",
        "\n",
        "Since downloading and clustering on the original set of images was quite slow, we decided to subsample this set. We uploaded this subsample, which contains 50 images per landmark, to Google Drive as a .zip file. By running the next cell, you can download and unzip this file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywOrieMy6xq0",
        "outputId": "8c7bcd0c-c22f-4273-dbd0-06c159f73936"
      },
      "outputs": [],
      "source": [
        "import gdown, zipfile, os, shutil\n",
        "\n",
        "# The file id on the Drive\n",
        "id = '1YbI4E82Ag_yY6Z2cM1x9QmCX8QUV0X8X'\n",
        "url='https://drive.google.com/uc?id={}'.format(id)\n",
        "\n",
        "# The output zip file\n",
        "output = 'paris_sample.zip'\n",
        "# Now download the file\n",
        "gdown.download(url,output,quiet=False)\n",
        "# Unzip the files\n",
        "#os.path.abspath(output)\n",
        "with zipfile.ZipFile(os.path.abspath(output), 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in different directory\n",
        "   zipObj.extractall('paris')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiRMpYH4ssUo"
      },
      "source": [
        "Now we have a folder called 'Paris'! \n",
        "We can proceed to perfoming our initial loading of the images in that folder. Run the following three code cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Jzw2RCfBKV"
      },
      "outputs": [],
      "source": [
        "def get_all_filepaths(directory):\n",
        "  '''\n",
        "  A helper function to get all absolute file paths in a directory (recursively)\n",
        "  :param directory:  The directory for which we want to get all file paths\n",
        "  :return         :  A list of all absolute file paths as strings\n",
        "  '''\n",
        "  for dirpath,_,filenames in os.walk(directory):\n",
        "    for f in sorted(filenames):\n",
        "      yield os.path.abspath(os.path.join(dirpath, f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDb9rj5Z_ieV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def load_sample_images(sample_pathnames):\n",
        "  '''\n",
        "  Initial loading the given images. \n",
        "  :param sample_pathnames: An array of image file paths that need to be opened\n",
        "  :return:                 A dictionary of the form key:image_dictionary,\n",
        "                           whereby image_dictionary itself is a dictionary \n",
        "                           containing the original image and the preprocessed\n",
        "                           image for later steps\n",
        "  '''\n",
        "  sample_images = {}\n",
        "  for filename in sample_pathnames:  # Loop through all images, load each file\n",
        "    sample_images[filename] = {}\n",
        "    sample_images[filename]['original'] = Image.open(filename)\n",
        "    sample_images[filename]['cv2'] = cv2.imread(filename)\n",
        "  return(sample_images)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfnUbjZ5fOpq",
        "outputId": "54a940f4-890a-4416-9a95-41387c56a47b"
      },
      "outputs": [],
      "source": [
        "sample_pathnames = sorted(list(get_all_filepaths('paris')))\n",
        "sample_images = load_sample_images(sample_pathnames)\n",
        "print('We have loaded ' + str(len(sample_pathnames)) + ' images!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLjx00HaVXso"
      },
      "source": [
        "An explanation of how you can access information about loaded images: \n",
        "\n",
        "***sample_pathnames*** is a list that contains all absolute file paths as strings. Filenames can be accessed by an index (a value between 0 and 599 since we have 600 images)\n",
        "\n",
        "***sample_images*** is a dictionary. The sample images' file paths are the keys for that dictionary. This means that you can access the information of a specific image by using that image's file path as a key. What you will then get is again a dictionary. At this point in the assignment, this dictionary has two possible keys, '*original*' and '*cv2*'. \n",
        "*   The key '*original*' is the string that accesses the image loaded using the PIL library. \n",
        "*   The key '*cv2*' is the string that accesses the image loaded using the cv2 library. We will use this second format for one type of image feature representation below.\n",
        "\n",
        "Please find usage examples below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P50eX1BVXCPz",
        "outputId": "a7a47ee4-0220-4d9d-c62e-650f00288574"
      },
      "outputs": [],
      "source": [
        "# Get filename of the image with index 2\n",
        "filename = sample_pathnames[2]\n",
        "print('This is our filename: {}'.format(filename))\n",
        "\n",
        "# Get the dictionary for that image\n",
        "print('This is the dictionary for this image:')\n",
        "print(sample_images[filename])\n",
        "\n",
        "# Get the image loaded using PIL:\n",
        "print('The image when loaded using PIL:')\n",
        "print(sample_images[filename]['original'])\n",
        "\n",
        "# Get the image loaded using cv2:\n",
        "print('The image when loaded using cv2:')\n",
        "print(sample_images[filename]['cv2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMiT-JHIB_X"
      },
      "source": [
        "### First impression\n",
        "\n",
        "Let's take a look at each landmark's first image to get a feeling for our dataset.\n",
        "Please note: To access the information we loaded for each image, we have to use the full path name of that image as a key in our sample_images dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "U5kk76ku_3hx",
        "outputId": "f004bb6c-8d0e-4fbe-ea80-3de739fd6cf1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12)) \n",
        "for i in range(0,12): \n",
        "  subplot = fig.add_subplot(4,3,i+1)\n",
        "  imgplot = plt.imshow(sample_images[sample_pathnames[i*50]]['original'])\n",
        "  subplot.set_title(sample_pathnames[i*50].split('/')[-2])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjZOQdNPM7--"
      },
      "source": [
        "## Clustering on simple features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAi4IJijNA2r"
      },
      "source": [
        "The images are loaded and we can start extracting some interesting properties from them to use as feature representations. \n",
        "In this subsection, we start with simple, straight-forward image features. The function *create_features_all_samples* below will be used from now on to create feature vectors for all images at once. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-3YriT0cQeC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def create_features(feature_funcs, image_dict):\n",
        "  '''\n",
        "  Loops over function names and calls each function and applies them on image\n",
        "  :param feature_funcs:   A list of functions that extract individual features\n",
        "  :param image_dict:       The loaded image dictionary to extract features from\n",
        "  :return:                 The feature vector for the image\n",
        "  '''\n",
        "  feature_vector = []  \n",
        "  for func in feature_funcs: # Subsequently apply each function\n",
        "    feature_vector.extend(func(image_dict)) \n",
        "  return feature_vector  \n",
        "\n",
        "\n",
        "def create_features_all_samples(feature_funcs,verbose=False):\n",
        "  '''\n",
        "  Loops over all sample images and calls create_features for each of them.\n",
        "  :param feature_funcs:   A list of functions that extract individual features \n",
        "  :param verbose:         True if print statements of progress are desired\n",
        "  :return:                All feature vectors combined in an array\n",
        "  '''\n",
        "  feature_vectors = []\n",
        "  start = time.time()\n",
        "  counter = 0 # Used to compute progress\n",
        "  for sample_pathname in sample_pathnames:\n",
        "    feature_vectors.append(create_features(feature_funcs, sample_images[sample_pathname]))\n",
        "    counter = counter +1\n",
        "    if verbose:\n",
        "      if counter%25 == 0:\n",
        "        print('Number of samples with features extracted: {}'.format(counter))  \n",
        "        print(\"Time elapsed for last 25 samples: {}\".format(time.time() - start))\n",
        "        start = time.time()\n",
        "  return feature_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlF7FLStNr0J"
      },
      "source": [
        "### Simple feature 1: Average color\n",
        "\n",
        "The first feature we will extract is an image's average color.\n",
        "\n",
        "Inspecting the format of an image loaded using PIL, you  already saw that the image was an **RGB** image. \n",
        "In the RGB format, a color value is represented by the intenisty of three colors: red **(R)**, green **(G)**, and blue **(B)**. Intensities are values between 0 and 255. \n",
        "\n",
        "The cell below shows nine example RGB color values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "BBLqoErgRfrY",
        "outputId": "fcc1af39-e21e-4866-bc8b-3a327d7bd17a"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 3)\n",
        "fig.tight_layout()\n",
        "\n",
        "# Add 9 example colors\n",
        "axs[0, 0].imshow([[(255, 0, 0)]])\n",
        "axs[0, 0].set_title('(255,   0,   0)')\n",
        "axs[0, 1].imshow([[(0, 255,   0)]])\n",
        "axs[0, 1].set_title('(  0,   255,   0)')\n",
        "axs[0, 2].imshow([[(0, 0, 255)]])\n",
        "axs[0, 2].set_title('(  0,   0, 255)')\n",
        "\n",
        "axs[1, 0].imshow([[(0, 0, 0)]])\n",
        "axs[1, 0].set_title('(  0,   0,   0)')\n",
        "axs[1, 1].imshow([[(127, 127, 127)]])\n",
        "axs[1, 1].set_title('(127,   127,   127)')\n",
        "axs[1, 2].imshow([[(255, 255, 255)]])\n",
        "axs[1, 2].set_title('(255, 255, 255)')\n",
        "\n",
        "axs[2, 0].imshow([[(255, 0, 255)]])\n",
        "axs[2, 0].set_title('(255,   0, 255)')\n",
        "axs[2, 1].imshow([[(255, 255, 0)]])\n",
        "axs[2, 1].set_title('(255,   255, 0)')\n",
        "axs[2, 2].imshow([[(240, 128, 128)]])\n",
        "axs[2, 2].set_title('(240,   128, 128)')\n",
        "\n",
        "\n",
        "# Remove the ticks \n",
        "for row in axs:\n",
        "  for ax in row:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwwK5Q4cIVS"
      },
      "source": [
        "In an **RGB image**, every pixel's color is represented by an RGB color value. \n",
        "An RGB image has three **color channels** (a red, a green and a blue channel). A color channel contains all pixels' intensity valuea for that color. For example, the red color channel contains all pixels' red intensity values (i.e. a number between 0 and 255 for each pixel). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0IjPq4w-pA"
      },
      "source": [
        "#### Task 1: Implement the function *average_color* in the cell below using the information provided in the comments. \n",
        "Hint: Remember that the original image can be accessed via *image_dict['original']*, and that you can convert this original image to a numpy array. (As a sanity check, make sure that you understand why the vector representation consists of three components in this case.)\n",
        "\n",
        "Don't forget to copy your function to the answer file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V65QaYjuSWHW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def average_color(image_dict):\n",
        "  '''\n",
        "  A function to compute the average RGB value of an image.\n",
        "  First, average over rows to obtain an average value per column.\n",
        "  Then, average over the resulting values to obtain one average value per color \n",
        "  channel.\n",
        "  \n",
        "  :param image_dict:  The dictionary containing the loaded image \n",
        "  :return:            A 3-dimensional np array: 1 average per color channel\n",
        "  '''\n",
        "  img = image_dict['original']\n",
        "  np_img = np.array(img)\n",
        "  rows,columns,channels =np_img.shape\n",
        "  red = 0\n",
        "  blue = 0\n",
        "  green = 0\n",
        "  pix_total = rows*columns\n",
        "  for i in range(rows):\n",
        "      for j in range(columns):\n",
        "          k = np_img[i,j]\n",
        "          if k[0] > k[1] and k[0] > k[2]:\n",
        "            red = red + 1\n",
        "            continue\n",
        "          if k[1] > k[0] and k[1] > k[2]:\n",
        "            green = green + 1\n",
        "            continue\n",
        "          if k[2] > k[0] and k[2] > k[1]:\n",
        "            blue = blue + 1 \n",
        "  res = np.array([red/pix_total,green/pix_total,blue/pix_total])\n",
        "  print(res)\n",
        "  return res\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeNVLEZUneF"
      },
      "source": [
        "Let's see what the result of your function looks like. \n",
        "We can use an arbitrary sample image, e.g. the image with index 102:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "WN_UKsEkNMWZ",
        "outputId": "ab9b3eff-6adc-42e9-aeca-1ff6f9e83de2"
      },
      "outputs": [],
      "source": [
        "# Extract the average color\n",
        "test_image_name = sample_pathnames[555] # Change the index here to see more examples\n",
        "test_image_dict = sample_images[test_image_name] \n",
        "test_image = test_image_dict['original']\n",
        "avg_color = average_color(test_image_dict).astype(int) # Cast for plotting\n",
        "\n",
        "# Plot the test image next to the average color\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.subplot(121), plt.imshow(test_image)\n",
        "plt.title('Image: {}'.format(test_image_name)), plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122), plt.imshow(Image.new(\"RGB\", (test_image.size[0],test_image.size[1]),\\\n",
        "                  (avg_color[0], avg_color[1], avg_color[2])))\n",
        "plt.title('The average color'), plt.xticks([]), plt.yticks([])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKoVPGQkroJp"
      },
      "source": [
        "#### **Clustering on average color**\n",
        "\n",
        "Now that we can successfully extract our first feature from images, it's time to perform some actual clustering on our dataset. \n",
        "Let's create a small feature vector for each sample image by extracting the average color:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X262uwDJcjyn",
        "outputId": "02600f82-5a1c-42cf-d32b-cf8cbf5cbc01"
      },
      "outputs": [],
      "source": [
        "avg_color_feature_vectors = np.asarray([create_features([average_color], sample_images[x]) for x in sample_pathnames])\n",
        "print('The dimensions of our array: {}'.format(avg_color_feature_vectors.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjOpZuw9ArA0"
      },
      "source": [
        "As you can see, we have 600 images, each represented by a vector containing 3 components.\n",
        "\n",
        "The function perform_k_means_clustering is where the clustering magic will happen. Run the two cells below to perform our first round of clustering (with 6 clusters) on the data using the small feature vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWjSoI1z4ngH"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def perform_k_means_clustering(feature_vectors, n_clusters=6):\n",
        "  '''\n",
        "  This function performs the clustering for us! It returns the n clusters.\n",
        "  :param feature_vectors: The feature vectors that represent our data samples.\n",
        "  :param n_clusters:      The number of clusters we want at the end (i.e. k)\n",
        "  '''\n",
        "  kmeans = KMeans(n_clusters=n_clusters,random_state=42).fit(feature_vectors)\n",
        "  y_kmeans = kmeans.predict(feature_vectors)\n",
        "  return y_kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoYgrGpTbifZ",
        "outputId": "d3da9ee7-be59-4ba9-cf50-8e02e0fa1b6d"
      },
      "outputs": [],
      "source": [
        "# Hint: In the function above, n_clusters is set to 6 by default. \n",
        "y_kmeans_avg_color = perform_k_means_clustering(avg_color_feature_vectors) \n",
        "print(\"This is the clustering algorithm's output: \")\n",
        "print(y_kmeans_avg_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpuxth_cBBC1"
      },
      "source": [
        "As you can see, the function outputs a list that assigns each image to one of  the clusters. The indices between 0 and 5 each stand for one of the 6 clusters. We can illustrate this clustering outcome utilizing an interactive 3d scatter plot. Run the following two cells to create this plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA8J_sCdo9ES"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def print_interactive_3d_cluster(features, feature_names, y_kmeans, filenames):\n",
        "  '''\n",
        "  A function to create an interactive (i.e. turnable) 3d scatterplot from our features.\n",
        "  Importantly, this can only be done successfully if we only have 3 dimensions. \n",
        "  :param features:       A 3d vector \n",
        "  :param feature_names:  The names of the 3 features\n",
        "  :param y_kmeans:       Our clustering algorithm's output (a list of integers)\n",
        "  :param filenames:      Our images' filenames\n",
        "  '''\n",
        "  if not (features.shape[1]<3 or len(feature_names)!=3): # Check if dimensions fit\n",
        "    # The following lines are a hack since plotly needs a pandas df as input\n",
        "    helper = pd.DataFrame()\n",
        "    helper[feature_names[0]] = features[:,0]\n",
        "    helper[feature_names[1]] = features[:,1]\n",
        "    helper[feature_names[2]] = features[:,2]\n",
        "    helper['cluster_index'] = [str(clusternum) for clusternum in y_kmeans]\n",
        "    helper['filenames'] = filenames\n",
        "    # Create our scatter plot\n",
        "    fig = px.scatter_3d(helper, x=feature_names[0], y=feature_names[1], z=feature_names[2], \n",
        "                       color = 'cluster_index', hover_data = ['filenames'])  \n",
        "    fig.show()\n",
        "  else:     \n",
        "    print('Sorry. This function only works for 3-dimensional feature vectors..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "eu6eiVL3-DTX",
        "outputId": "f8cdca25-044c-4076-bfe6-7469f45c2a6d"
      },
      "outputs": [],
      "source": [
        "fig = print_interactive_3d_cluster(avg_color_feature_vectors,['red_average','green_average','blue_average'], y_kmeans_avg_color,sample_pathnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvmITTMFuDlX"
      },
      "source": [
        "However, we also want to see which images are in those clusters. Running the next two lines will enable you to automatically create and display an image montage for each cluster. This montage contains the images that are in that cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgzNVqeLnCrG",
        "outputId": "1fa594a1-7098-4a33-a2ac-0f017851e9e6"
      },
      "outputs": [],
      "source": [
        "# You might have to restart your runtime after this line executes\n",
        "!pip install --upgrade imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3wdULSgoq0t"
      },
      "outputs": [],
      "source": [
        "from imutils import build_montages\n",
        "from google.colab.patches import cv2_imshow \n",
        "\n",
        "def show_images_in_clusters(y_kmeans, sample_pathnames, sample_images):\n",
        "  '''\n",
        "  Create a montage for each cluster in y_kmeans featuring images in that cluster\n",
        "  and subsequently display those montages. \n",
        "  :param y_kmeans:          The output vector of a clustering algorithm containing \n",
        "                            cluster indices for each sample\n",
        "  :param sample_pathnames:  Our filenames list\n",
        "  :param sample_images:     Our image dictionary \n",
        "  '''\n",
        "  colnum = 9 # Specified by us, 9 columns looked the nicest.\n",
        "  for cluster_index in np.unique(y_kmeans):\n",
        "    montage_images = [np.asarray(sample_images[sample_pathnames[index]]['original']) \\\n",
        "                      for index, value in enumerate(y_kmeans) if value == cluster_index]\n",
        "    rownum = int(len(montage_images)/colnum) # Calculate the number of rows we'll need\n",
        "    if rownum == 0: rownum =1\n",
        "    montages = build_montages(montage_images, (128, 196), (colnum, rownum))[0]\n",
        "    fig = plt.figure(figsize=(30, 30))\n",
        "    plt.title('Cluster index: {}'.format(cluster_index)), plt.xticks([]), plt.yticks([])\n",
        "    plt.imshow(montages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdXVTLvvKGA"
      },
      "source": [
        "It's time to take a look at the images in our first clusters! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n89lxO6wOLQA",
        "outputId": "dc35eaa5-1008-4f44-c35c-706bc5a6ab66"
      },
      "outputs": [],
      "source": [
        "show_images_in_clusters(y_kmeans_avg_color, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnz47VKRqcNY"
      },
      "source": [
        "Hint: As it is a bit annoying to scroll within a notebook, we would like to point you to the full screen viewing mode. You can find it by clicking on the top right corner of the cell with the image output (three dots -> view output fullscreen)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZHLZIJvX-VI"
      },
      "source": [
        "### Simple feature 2: Color histograms\n",
        "\n",
        "Instead of having one average value per color channel, so-called color histograms are a commonly used image feature. Let's take some minutes to understand the underlying idea behind this feature. For this purpose, we will zoom into an example image and investigate a tiny 15x15 pixel subarea:  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "Q0VXNJcqZytt",
        "outputId": "d712977f-ccdd-42a1-f56f-97a513adb574"
      },
      "outputs": [],
      "source": [
        "test_image = Image.open('/content/paris/louvre/paris_louvre_000014.jpg')\n",
        "# Crop the image -> zoomed-in view on lower right pixel values\n",
        "w, h = test_image.size\n",
        "subpart_test_image = np.array(test_image.crop((w-15, h-15, w, h)))\n",
        "\n",
        "# Plot the image and the cropped subpart of the image\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.subplot(121), plt.imshow(test_image)\n",
        "plt.title('Our original image')\n",
        "plt.subplot(122), plt.imshow(subpart_test_image)\n",
        "plt.title('Cropped to 15x15 pixels')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igLIU2FAaLv7"
      },
      "source": [
        "As said above, we don't just want one average value for each color channel. Instead, we assign each pixel value of a color channel to a bin. Binning means that we group together values that fall within a certain interval. In our case, each bin represents a particular range of pixel values. Here, we use three bins. If a pixel's value falls within the first third of possible values, it is assigned to the first bin (index 0). If it is within the second third, it is assigned to the second bin (index 1), and if it is within the final third it is assigned to the third bin (index 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FyudSRK3bUpq",
        "outputId": "ae55e51c-ee07-4bd2-89e3-8864c3303e67"
      },
      "outputs": [],
      "source": [
        "# Create one image for each color channel:\n",
        "for i in range(3):\n",
        "  channel_img = np.zeros(subpart_test_image.shape, dtype='uint8')\n",
        "  channel_img[:,:,i] = subpart_test_image[:,:,i]\n",
        "  # First plot the color channel with the actual pixel values\n",
        "  px_values = channel_img[:,:,i]\n",
        "  plt.figure(figsize=(14,14))\n",
        "  ax = plt.subplot(1,2,1)\n",
        "  im = ax.imshow(channel_img)\n",
        "  # Put the pixel values as text on top of the image:\n",
        "  for j in range(channel_img.shape[0]):\n",
        "    for k in range(channel_img.shape[1]):\n",
        "      text = ax.text(k, j, px_values[j, k], ha=\"center\", va=\"center\", color=\"w\")\n",
        "  plt.title('Pixel values')\n",
        "  # Now plot the color channel with the bin values\n",
        "  bin_values = np.floor_divide(px_values,255/3).astype(int)\n",
        "  ax = plt.subplot(1,2,2)\n",
        "  im = ax.imshow(channel_img)\n",
        "  for j in range(channel_img.shape[0]):\n",
        "    for k in range(channel_img.shape[1]):\n",
        "      text = ax.text(k, j, bin_values[j, k], ha=\"center\", va=\"center\", color=\"w\")\n",
        "  plt.title('Bin values')\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvrmUxmxbfan"
      },
      "source": [
        "As you can see, each pixel has been assigned to one bin for every color channel. This leaves us with 3x3x3 = 27 possible combinations of the individual red, green and blue bin values: \n",
        "\n",
        "| R | G | B |\n",
        "| --- | --- | --- |\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 0 | 1 |\n",
        "| 0 | 0 | 2 |\n",
        "| 1 | 0 | 0 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 0 | 2 |\n",
        "| 2 | 0 | 0 |\n",
        "| 2 | 0 | 1 |\n",
        "| 2 | 0 | 2 |\n",
        "| 0 | 1 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 0 | 1 | 2 |\n",
        "| 1 | 1 | 0 |\n",
        "| 1 | 1 | 1 |\n",
        "| 1 | 1 | 2 |\n",
        "| 2 | 1 | 0 |\n",
        "| 2 | 1 | 1 |\n",
        "| 2 | 1 | 2 |\n",
        "| 0 | 2 | 0 |\n",
        "| 0 | 2 | 1 |\n",
        "| 0 | 2 | 2 |\n",
        "| 1 | 2 | 0 |\n",
        "| 1 | 2 | 1 |\n",
        "| 1 | 2 | 2 |\n",
        "| 2 | 2 | 0 |\n",
        "| 2 | 2 | 1 |\n",
        "| 2 | 2 | 2 |\n",
        "\n",
        "Our histogram is built by counting the number of pixels that belong to each of these combinations. In practice, you then normalize these counts to account for different sizes of images. This yields a normalized color histogram feature vector with 27 elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U87Q8nH1Q3VE"
      },
      "source": [
        "The following function was taken from a nice [blogpost](https://gogul.dev/software/image-classification-python) and adjusted for our purpose. It computes exactly this feature vector for a given image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnPBvnPQmZmG"
      },
      "outputs": [],
      "source": [
        "def color_histogram(image_dict):\n",
        "  '''\n",
        "  Compute the normalized color histogram binned into 3x3x3 bins from the HSV image. \n",
        "  :param image_dict:  The dictionary containing the loaded image \n",
        "  :return:            A 27-dimensional np array\n",
        "  '''\n",
        "  # extract a 3D color histogram from the RGB color space\n",
        "  im = image_dict['cv2']\n",
        "  hist = cv2.calcHist([im], [0, 1, 2], None, [3,3,3], [0, 256, 0, 256, 0, 256])\n",
        "  # normalize the histogram\n",
        "  hist = cv2.normalize(hist,hist)\n",
        "  # return the flattened histogram as the feature vector\n",
        "  return hist.flatten() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xssByohWzhY4"
      },
      "source": [
        "Let's see what the output for one vector looks like:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDnbx9yHpgnU",
        "outputId": "ebddc793-0727-48d0-db55-c46a10b7a32f"
      },
      "outputs": [],
      "source": [
        "test_chist_vector = color_histogram(test_image_dict)\n",
        "test_chist_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDV7IPcxdgTI"
      },
      "source": [
        "**Clustering on color histograms**\n",
        "\n",
        "See? We have our 27 element feature vector. Now we can perform clustering on our dataset using color histograms as features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaY3ZCC6sqrb"
      },
      "outputs": [],
      "source": [
        "chist_feature_vectors =  create_features_all_samples([color_histogram],verbose=False)\n",
        "y_kmeans_chist = perform_k_means_clustering(chist_feature_vectors) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I82Y2sYxeE2y"
      },
      "source": [
        "Unfortunately, we cannot use our 3d scatter plot anymore since we have a higher-dimensional feature vector. However, we can inspect our clusters using the montages: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j9gJ1BfjtCl5",
        "outputId": "4a1d3e48-6173-4fea-f4b6-94d25a444e5d"
      },
      "outputs": [],
      "source": [
        "show_images_in_clusters(y_kmeans_chist, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqUHCOFDB2QW"
      },
      "source": [
        "In some cases, three bins for each color channel might not be enough to represent semantic content in a dataset. In practice, researchers use 32 bins or more, depending on the problem. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc6L3ccyJJJL"
      },
      "source": [
        "#### Task 2: Change the color histogram function below so that it uses 32 bins per color channel. Then run the second cell and inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG_HKz3mB3dl"
      },
      "outputs": [],
      "source": [
        "def color_histogram_32bins(image_dict):\n",
        "  '''\n",
        "  Compute the normalized color histogram binned into 32x32x32 bins from the RGB image. \n",
        "  :param image_dict:  The dictionary containing the loaded image \n",
        "  :return:            A 32768-dimensional np array\n",
        "  '''\n",
        "  # extract a 3D color histogram from the RGB color space\n",
        "  im = image_dict['cv2']\n",
        "  hist = cv2.calcHist([im], [0, 1, 2], None, [3,3,3], [0, 256, 0, 256, 0, 256])\n",
        "  # normalize the histogram\n",
        "  hist = cv2.normalize(hist,hist)\n",
        "  # return the flattened histogram as the feature vector\n",
        "  return hist.flatten() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F9kNnLptDmVS",
        "outputId": "49f570c3-b798-4673-b404-0ff3761385af"
      },
      "outputs": [],
      "source": [
        "chist_32bins_feature_vectors =  create_features_all_samples([color_histogram_32bins],verbose=False)\n",
        "y_kmeans_chist_32bins = perform_k_means_clustering(chist_32bins_feature_vectors) \n",
        "show_images_in_clusters(y_kmeans_chist_32bins, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBhtDB8j5xt"
      },
      "source": [
        "Our clustering function creates 6 clusters by default. We now want to see what happens if we increase the number of clusters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o1n4crhGYV4"
      },
      "source": [
        "#### Task 3: Perform clustering with 12 clusters on the *chist_32bins_feature_vectors* and display the montages for these clusters in the cell below. \n",
        "Then repeat the same step with 24 clusters in the second cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nXVY1Lyhj6IT",
        "outputId": "61cf5b28-51ca-4200-8c07-ed55586421fc"
      },
      "outputs": [],
      "source": [
        "y_kmeans_chist_12 = perform_k_means_clustering(chist_32bins_feature_vectors, n_clusters = 12)\n",
        "show_images_in_clusters(y_kmeans_chist_12, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSbPsfxooDD7",
        "outputId": "4617546f-bf0e-442c-c66a-fc86256ed1bf"
      },
      "outputs": [],
      "source": [
        "y_kmeans_chist_24 =perform_k_means_clustering(chist_32bins_feature_vectors, n_clusters = 24)\n",
        "show_images_in_clusters(y_kmeans_chist_24, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38D1jqQKuR4"
      },
      "source": [
        "A side note:\n",
        "In practice, instead of RGB, the HSV or CIELAB color spaces are mostly preferred. This is because they more closely reflect human color perception and the way we perceive differences between colors. With respect to the scope of this assignment, we decided to restrict ourselves to RGB here. Please be aware of the other color spaces in real-world projects. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlaGJsSBIAvw"
      },
      "source": [
        "### Simple feature 3: Histogram of Oriented Gradients (HOG)\n",
        "\n",
        "There is so much more to images than their colors!  Another simple but interesting image property is edges. Edges are typically characterized by a sudden dramatic change in color between pixels. Consider, for example, a black rectange on a white background. The rectangle's edges can be found by searching where pixel colors change from black to white. \n",
        "\n",
        "Here, we will be creating image representations based on edges. These involve using pixels' gradient vectors. Simply put, the gradient vector of each pixel in an image tells us how the color changes when moving from this pixel to its direct neighbour in a certain direction. This movement can happen in different directions: You can move left, right, up, down or along one of the four possible diagonals (upper/lower right and upper/lower left). These eight directions are known as oriented gradients. The histogram of these gradients tells us how strong these gradients are for a certain location in an image. Similarly to color histograms, HOGs use are created using binning (on orientations).\n",
        "\n",
        "Let's take a look at our example image again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "QY7i4qj8rxwU",
        "outputId": "a7f9d791-e760-4a83-a0e8-049d3b9071cb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,9))\n",
        "plt.imshow(test_image) , plt.xticks([]), plt.yticks([])\n",
        "plt.title('Test image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7APAn93eteXZ"
      },
      "source": [
        "We can visualize the test image's HOG by running the next cell. Luckily, there is a nice function called *hog* from the library *skimage* that can directly compute HOG for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "yMGW7VqfsrE1",
        "outputId": "b1fde0f4-b682-4ebd-8fa4-66b047180b5f"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "\n",
        "fd, hog_image = hog(test_image, orientations=8, pixels_per_cell=(16, 16),\n",
        "                    cells_per_block=(1, 1), visualize=True, multichannel=True, feature_vector=True)\n",
        "hog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "plt.imshow(hog_image, cmap=plt.cm.gray) , plt.xticks([]), plt.yticks([])\n",
        "plt.title('HOG')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ljihZ7V55fy"
      },
      "source": [
        "As you can see, the HOG is strongest at the edges in our test image. To perform clustering, we can now make a feature function using hog:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmTX3oYgIHlD"
      },
      "outputs": [],
      "source": [
        "def hog_vector(image_dict):\n",
        "  '''\n",
        "  Compute the histogram of oriented gradients binned into 8 bins. \n",
        "  :param image_dict:  The dictionary of loaded images for that image\n",
        "  :return:            A feature vector of length 8        \n",
        "  '''\n",
        "  img = image_dict['original'].resize((300,300), Image.ANTIALIAS)  \n",
        "  fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=True, multichannel=True, feature_vector=True)  \n",
        "  \n",
        "  return fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-ctGc9z7xt"
      },
      "source": [
        "Let's see what the HOG representation for one image looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGyvc6QBz9OU",
        "outputId": "9dcdacc2-80a1-4cd1-a723-5f40d7c4edda"
      },
      "outputs": [],
      "source": [
        "test_hog_vector = hog_vector(test_image_dict)\n",
        "test_hog_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlu9y-PE5Zbf"
      },
      "source": [
        "Our feature function only returns the feature representation (not the image we displayed above), which is a vector containing 8 elements (1 per orientation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJRqgZmR6r4V"
      },
      "source": [
        "**Clustering on HOG**\n",
        "\n",
        "You might guess it already: It's time to perform our clustering! Since we considered 8 different directions of gradients, why don't we try producing 8 clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hp87wjOa6jP4",
        "outputId": "bbad57c8-7acb-4916-e9d9-03c31d96bb42"
      },
      "outputs": [],
      "source": [
        "hog_feature_vectors =  create_features_all_samples([hog_vector],verbose=False)\n",
        "y_kmeans_hog = perform_k_means_clustering(hog_feature_vectors, n_clusters=8)\n",
        "show_images_in_clusters(y_kmeans_hog, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15S4C2A_5j8"
      },
      "source": [
        "## Clustering using neural representations\n",
        "\n",
        "Neural networks can be used to create image representations that are capable of capturing image semantics more directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2JcnSAKYfuS"
      },
      "source": [
        "### Introducing neural representations\n",
        "\n",
        "How this works: We use a neural network that has been trained to perform well on a huge image recognition task. In this assignment, the network we use is called 'VGG19'. The last layers in the network are responsible for classification, but other layers in the network can be used to extract feature representations. These feature representations are useful for clustering (and also classification, by the way) because they '...provide a high-level descriptor of the visual content of the image' ([Babenko et al. 2014](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_38)).\n",
        "\n",
        "Specifically, we can put our images into the VGG19 network and use the *fc2* layer as output layer.  With the command *summary*, we can inspect the architecture of the model at hand.  Let's take a look at the VGG19 network's original architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSMzaJYNAcXD",
        "outputId": "7391aaee-7023-4806-a92e-51b6853d5c6c"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "import tensorflow as tf\n",
        "# from keras.preprocessing import image\n",
        "from tensorflow.keras.utils import img_to_array \n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.models import Model\n",
        "\n",
        "original_model = VGG19(weights='imagenet')\n",
        "original_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As_HnrgEN6Wx"
      },
      "source": [
        "We have to remove only VGG19's *predictions* layer to get the output from the *fc2* layer. Run the cell below to get rid of this layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGoGbHacUULh"
      },
      "outputs": [],
      "source": [
        "vgg19_without_last = Model(inputs=original_model.inputs, outputs=original_model.get_layer('fc2').output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35tf_pKcYrse"
      },
      "source": [
        "Our images need to be reshaped, since the model expects images of size (244, 244). We can store the reshaped images in our *sample_images* dictionary by running the two cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBhaZND4wDC6"
      },
      "outputs": [],
      "source": [
        "def load_image_to_format(image_path, format=(224,224)):\n",
        "  '''\n",
        "  Crop the image before resizing it. \n",
        "  :param image_path: The path to the image\n",
        "  :return          : The resized image\n",
        "  '''\n",
        "  img = Image.open(image_path)\n",
        "  height,width = img.size\n",
        "  # fit in the biggest possible square to crop this\n",
        "  square_size = min(height,width)\n",
        "  # compute offsets\n",
        "  x_offset = height - square_size\n",
        "  y_offset = width - square_size\n",
        "  img = img.crop((x_offset, y_offset,x_offset + square_size,y_offset + square_size))\n",
        "  img = img.resize(format)\n",
        "  return np.array(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWih4FMyZJOW"
      },
      "outputs": [],
      "source": [
        "for filename in sample_pathnames:  \n",
        "  img = load_image_to_format(filename) \n",
        "  preprocessed_img = img_to_array(img)\n",
        "  preprocessed_img = np.expand_dims(preprocessed_img, axis=0)\n",
        "  preprocessed_img = preprocess_input(preprocessed_img)   \n",
        "  sample_images[filename]['preprocessed'] = preprocessed_img   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcPH_pFlswMD"
      },
      "source": [
        "We can now use the following function to extract neural_representations from a given image file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfWiA9PUs5B4"
      },
      "outputs": [],
      "source": [
        "def neural_representations(image_dict):\n",
        "  '''\n",
        "  A function that creates neural representations using the vgg19_without_last model.\n",
        "  :param image:   The dictionary of the image\n",
        "  :return:        A (flattened) numpy array containing the neural representations\n",
        "  '''\n",
        "  return np.array(vgg19_without_last.predict(image_dict['preprocessed'])).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ggpSY_BWd6"
      },
      "source": [
        "Let's test this out for a random example image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avA5R0JOBVWD",
        "outputId": "2f4f16dd-afc2-4413-cc41-c153c2280685"
      },
      "outputs": [],
      "source": [
        "print(neural_representations(sample_images[sample_pathnames[400]]).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf3iUJwhfkBa"
      },
      "source": [
        "As you can see in the model's original architecture above, the *fc2* layer gives us 4096 features.  We can perform our clustering using those features. Please note that this might take a while. To get an estimation of the duration, we will enable the function argument *verbose*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyacex1igwmr",
        "outputId": "e1702678-2916-4186-a4dc-edf6788d82b4"
      },
      "outputs": [],
      "source": [
        "neural_representation_feature_vectors = create_features_all_samples([neural_representations],verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyNMe8TgB-Ld"
      },
      "source": [
        "We're there! Now we can finally conduct the clustering. We will tell the algorithm to create 12 clusters, since we have 12 different landmarks reflected in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EESFjEAr0kmz",
        "outputId": "d9a96777-3942-427b-a2f6-85749fc8052d"
      },
      "outputs": [],
      "source": [
        "y_kmeans_neural_representations = perform_k_means_clustering(neural_representation_feature_vectors,n_clusters=12) \n",
        "show_images_in_clusters(y_kmeans_neural_representations, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J69pG6yCyeJ"
      },
      "source": [
        "#### Task 4: Now systematically vary the number of clusters in the cell below. Try out both more and fewer clusters. Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Y3jd40BDoFn",
        "outputId": "5d05400c-4f92-442a-f8db-7cf8df8754a6"
      },
      "outputs": [],
      "source": [
        "y_kmeans_neural_representations_n = perform_k_means_clustering(neural_representation_feature_vectors,n_clusters=18)  # Vary the number here\n",
        "show_images_in_clusters(y_kmeans_neural_representations_n, sample_pathnames, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGw0ypnIUe6X"
      },
      "source": [
        "\"Clustering bias\" refers to the perspective that we decide to take on image similarity when we design and implement our clustering. The four image representations we investigated in our clustering study represent four different possible \"clustering bias\". "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
